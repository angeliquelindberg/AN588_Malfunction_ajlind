---
title: "Homework 4: What's Your Malfunction"
author: "Angelique J. Lindberg"
date: "`r Sys.Date()`"
output:
  html_document: 
    theme: cerulean
    toc: true  
    toc_float: true  
---
<!-- Learned how to make a chunk of text a comment in R: Code > Comment/Uncomment Lines. It doesn't show up in the Knit version. Just wanted to share w/ you :) - Julianna -->
<!-- I also used the same theme!!  -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(curl) #PACKAGES NEEDED!
library(ggplot2)
```

# Question 1

Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:
- Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().
- When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().
The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.
-The function should contain a check for the rules of thumb we have talked about (n∗p>5
 and n∗(1−p)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.
-The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.
```{r eval = FALSE}
Z.prop.test<-function(p1, n1, p2=NULL, n2=NULL, p0, alternative="two.sided", conf.level=0.95){
if(is.null(p2))  
  Z<-(p1-p0)/(sqrt((p0(1-p0))/n))
  P<-2*(1-pnorm(Z))
  CI<-c(lower,upper)
    lower<-p1-qnorm(0.975) * sqrt(p1*(1-p1)/n1) #these won't calculate correctly with a different confidence level, don't know how to address that, gonna just add an error for that
    upper<-phat+qnorm(0.975) * sqrt(p1*(1-p1)/n1)
    return(list(Z,P,CI))
}else { #my else statement is not working
            Z<-(p1-p2-p0)/(sqrt((pstar(1-pstar))(1/n1+1/n2)))
              pstar<-(x1+x2)/(n1+n2)
            P<-1 - pnorm(z)
            CI<-c(lower2, upper2)
            lower2<-(p2-p1)- qnorm(0.975) * sqrt(p1*(1-p1)/n1) * sqrt(p2*(1-p2)/n2)
            upper2<-(p2-p1) + qnorm(0.975) * sqrt(p1*(1-p1)/n1) * sqrt(p2*(1-p2)/n2)
             #  pstar - but don't know how to know the numerator (number of successes), that isn't a defined argument to pass, I could add it?
          return(list(Z,P,CI))
}
if (conf.level != 0.95){
   stop("can not compute unless conf.level is 0.95")
}
if (p1* n1 < 5 || n1(1-p1)>5){ #rule of thumb check
   print("Warning: Test violates assumptions.")
}
if (is_not_null(p2) && p2*n2 <5 || n1(1-p2)<5) {
     print("Warning: Test violates assumptions.")
}
```


# Question 2
The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. 

Okay, let's load the Kamilar and Cooper Data
```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
kc <- read.csv(f, header = TRUE, stringsAsFactors = FALSE)
attach(kc)
```

Do the following for both longevity~brain size and log(longevity)~log(brain size):

- Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

## Regression model and interpretation longevity~brain size

Okay, for regression models we want to use lm() (or lmodel2(), but we ended up getting comparable answers, so let's go with lm()).
```{r}
lbmodel<-lm(data=kc, MaxLongevity_m~Brain_Size_Species_Mean) 
summary(lbmodel)
```
```{r}
lbmplot<-ggplot(data=kc, aes(x=MaxLongevity_m, y=Brain_Size_Species_Mean)) 
lbmplot<-lbmplot+geom_point() #creating scatterplot
lbmplot<-lbmplot+geom_smooth(method = "lm", formula = y ~ x) #adding fitted line on top
eq<-"y=1.22x+248.95"
lbmplot<-lbmplot+annotate("text", x= 600, y= 400, label="y=1.22x+248.95") #geom_text didn't work so I used annotate #equation was written based on lm() results from previous chunk
lbmplot
```

- Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

```{r}
summary(lbmodel)
```
Okay, the intercept (or beta1) is identified by intercept row and it is estimated by the model to be 248.95. This means that the expected value of the longevity of a species  with a brain size of 0 grams is 248.95 months.

The slope (or beta0) is identified by the predictor or "x" value in the regression model summary - the brain_size_species_mean row - and it is estimated to be 1.22. This suggests that a species' longevity increases 1.22 months for each 1 gram increase in brain size.

The outcome of the test is that about 49% of the variation in longevoty is explained by species' brain size. The null hypothesis (H0: β1 = 0) is rejected and the alternate hypothesis (HA: β1 ≠ 0) is accepted since some of the variation is explained by the predictor, the slope is positive, aand because all the p-values are quite significant. Thus, brain size does have some effect on longevity.

To do confidence interval of slope, I'm going to use confint().
```{r}
ci<-confint(lbmodel, level=0.90)
ci
```
The 90% confidence interval of the slope is (1.03, 1.4). This means that there's a 90% confidence that the true slope is between 1.03 and 1.4.

- Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines

``{r eval=false}
df<-data.frame(kc$Brain_Size_Species_Mean)
cinterv<- predict(lbmodel, newdata=kc, interval = "confidence",
    level = 0.90)
df<-cbind(df,cinterv)
names(df)<-c("xsizes", "cintervfit", "cintervlwr", "cinterupr")
lbmplot<-lbmplot + geom_line(aes(x = xsizes, y = cintervlwr, colour = "black"))
lbmplot
``

^^ 
this is me trying to follow the Module 12 example and it not working, I tried googling for help but all I could find was the geom_smooth confidence intervals from earlier 
if it did work, I would do the same for prediction using below equation:
pinterval<-pi <- predict(lbmodel, newdata =kc, interval = "prediction",
    level = 0.90)
    
- Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

Okay, we're going to use predict(), like for the intervals, but specify the size we want estimated.
```{r}
cipoint <- predict(lbmodel, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction",  level = 0.90)  # for a single value
cipoint
```

I don't necessarily trust this estimate since the r-squared was only 0.49, so the brain size isn't that determinative of longevity by itself.

## Regression Model and interpretation log(longevity)~log(brain size)

Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

```{r}
kc$loglongevity<-log(kc$MaxLongevity_m)
kc$logbrainsize<-log(kc$Brain_Size_Species_Mean)
loglbmodel<-lm(data=kc, loglongevity~logbrainsize)
summary(loglbmodel)
```
```{r}
loglbmplot<-ggplot(data=kc, aes(x=loglongevity, y=logbrainsize)) 
loglbmplot<-loglbmplot+geom_point() #creating scatterplot
loglbmplot<-loglbmplot+geom_smooth(method = "lm", formula = y ~ x) #adding fitted line on top
logeq<-"y=0.23x+4.88"
loglbmplot<-loglbmplot+annotate("text", x= 5, y= 5, label="y=0.23x+4.88") #geom_text didn't work so I used annotate #equation was written based on lm() results from previous chunk
loglbmplot
```


- Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

```{r}
summary(loglbmodel)
```

The intercept (beta1), identified by intercept row, is estimated to be 4.88. This means that the expected value of the logarithm of the longevity of a species with a logarithmic brain size of 0 grams is 4.88 months.

The slope (beta0) - the logbrainsize row - is estimated to be 0.23. This suggests that the logarithm of species' longevity increases by 1.22  for each 1  increase in the logarithm of species' brain size.

The outcome of the test is that about 25% of the variation in the logarithm of longevity is explained by the logarithm of species' brain size. The null hypothesis (H0: β1 = 0) is rejected and the alternate hypothesis (HA: β1 ≠ 0) is accepted since some of the variation is explained by the predictor, the slope is positive, and because all the p-values are quite significant. Thus, the log of brain size does have some effect on the log of longevity. However, 25% isn't that high and the slope is quite low, so I wouldn't be very comfortable or confident in interpreting how significant the association is.

The confidence interval for the slope:
```{r}
logci<-confint(loglbmodel, level=0.90)
logci
```
- Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines

(-_-)Don't know how to do this :=( I tried above but not working

- Looking at your two models, which do you think is better? Why?
Well, I had a hard time explaining what the coefficients and the results meant for the log model. 

```{r}
par(mfrow = c(1, 2))
lbmplot
loglbmplot
```

I do think the log model looks better graphically because the relationship looks much clearer.

#Challenges
-Well, the elephants in the room are that first question and trying to add the confidence and prediction intervals. I tried following the module a dozen different ways but the x argument in my geom_line just wouldn't work, it thinks it's not the right length for some reason, I tried defining it as the column from the data frame, extracting that column by itself as a data frame, adding it to an existing data frame, and nope. Also tried changing the name of the column, using quotes, all of that. 
-Geom_text doesn't work for labelling for me, it creates a label for each point so it's repeating a bajillion times across the graph and ruining it. 